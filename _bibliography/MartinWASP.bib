Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Nishio2018,
abstract = {We envision a mobile edge computing (MEC) framework for machine learning (ML) technologies, which leverages distributed client data and computation resources for training high-performance ML models while preserving a client privacy. Toward this future goal, this work aims to extend Federated Learning (FL), which enables privacy-preserving training of models, to work with heterogeneous clients in a practical cellular network. The FL protocol iteratively asks random clients to download a trainable model from a server, update it with own data, and upload the updated model to the server, while asking the server to aggregate multiple client updates to further improve the model. While clients in this protocol are free from disclosing own private data, the overall training process can become inefficient when some clients are with limited computational resources (i.e., requiring longer update time) or under poor wireless channel conditions (longer upload time). Our new FL protocol, which we refer to as FedCS, mitigates this problem and performs FL efficiently while actively managing clients based on their resource conditions. Specifically, FedCS solves a client selection problem with resource constraints, which selects the maximum possible number of clients who can complete the FL's download, update, and upload steps within a certain deadline. This selection strategy results in the server aggregating as many client updates as possible and accelerating performance improvement in ML models (e.g., classification accuracy.) We conducted an experimental evaluation using publicly-available large-scale image datasets to train deep neural networks on MEC environment simulations. The experimental results show that FedCS is able to complete its training process in a significantly shorter time compared to the original FL protocol.},
archivePrefix = {arXiv},
arxivId = {1804.08333},
author = {Nishio, Takayuki and Yonetani, Ryo},
eprint = {1804.08333},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Nishio, Yonetani{\_}2018{\_}Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge.pdf:pdf},
title = {{Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge}},
url = {http://arxiv.org/abs/1804.08333},
year = {2018}
}
@article{openimages,
author = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio and Abu-El-Haija, Sami and Kuznetsova, Alina and Rom, Hassan and Uijlings, Jasper and Popov, Stefan and Kamali, Shahab and Malloci, Matteo and Pont-Tuset, Jordi and Veit, Andreas and Belongie, Serge and Gomes, Victor and Gupta, Abhinav and Sun, Chen and Chechik, Gal and Cai, David and Feng, Zheyun and Narayanan, Dhyanesh and Murphy, Kevin},
journal = {Dataset available from https://storage.googleapis.com/openimages/web/index.html},
title = {{OpenImages: A public dataset for large-scale multi-label and multi-class image classification.}},
url = {https://storage.googleapis.com/openimages/},
year = {2017}
}
@article{Vanhaesebrouck2016,
abstract = {We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. To optimize this challenging objective, our decentralized algorithm is based on ADMM.},
archivePrefix = {arXiv},
arxivId = {1610.05202},
author = {Vanhaesebrouck, Paul and Bellet, Aur{\'{e}}lien and Tommasi, Marc},
eprint = {1610.05202},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Vanhaesebrouck, Bellet, Tommasi{\_}2016{\_}Decentralized Collaborative Learning of Personalized Models over Networks.pdf:pdf},
month = {oct},
title = {{Decentralized Collaborative Learning of Personalized Models over Networks}},
url = {http://arxiv.org/abs/1610.05202},
year = {2016}
}
@article{sander1991database,
author = {Sander, Chris and Schneider, Reinhard},
journal = {Proteins: Structure, Function, and Bioinformatics},
number = {1},
pages = {56--68},
publisher = {Wiley Online Library},
title = {{Database of homology-derived protein structures and the structural meaning of sequence alignment}},
volume = {9},
year = {1991}
}
@article{Finn2017,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
eprint = {1703.03400},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Finn, Abbeel, Levine{\_}2017{\_}Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:pdf},
month = {mar},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {http://arxiv.org/abs/1703.03400},
year = {2017}
}
@article{brendan2016communication,
abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
archivePrefix = {arXiv},
arxivId = {1602.05629},
author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Ag{\"{u}}era y and {Brendan McMahan}, H and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise},
eprint = {1602.05629},
file = {:Users/eisamar/Documents/Mendeley/arXiv preprint arXiv1602.05629/McMahan et al.{\_}2016{\_}Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf:pdf},
journal = {arXiv preprint arXiv:1602.05629},
title = {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
url = {http://arxiv.org/abs/1602.05629},
volume = {54},
year = {2016}
}
@article{Smith2016,
abstract = {The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.},
archivePrefix = {arXiv},
arxivId = {1611.02189},
author = {Smith, Virginia and Forte, Simone and Ma, Chenxin and Takac, Martin and Jordan, Michael I. and Jaggi, Martin},
eprint = {1611.02189},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Smith et al.{\_}2016{\_}CoCoA A General Framework for Communication-Efficient Distributed Optimization.pdf:pdf},
title = {{CoCoA: A General Framework for Communication-Efficient Distributed Optimization}},
url = {http://arxiv.org/abs/1611.02189},
year = {2016}
}
@inproceedings{Argyriou2006MultiTaskFL,
author = {Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
booktitle = {NIPS},
file = {:Users/eisamar/Documents/Mendeley/NIPS/Argyriou, Evgeniou, Pontil{\_}2006{\_}Multi-Task Feature Learning.pdf:pdf},
title = {{Multi-Task Feature Learning}},
year = {2006}
}
@article{Chen2018,
abstract = {This paper presents a new class of gradient methods for distributed machine learning that adaptively skip the gradient calculations to learn with reduced communication and computation. Simple rules are designed to detect slowly-varying gradients and, therefore, trigger the reuse of outdated gradients. The resultant gradient-based algorithms are termed $\backslash$textbf{\{}L{\}}azily $\backslash$textbf{\{}A{\}}ggregated $\backslash$textbf{\{}G{\}}radient --- justifying our acronym $\backslash$textbf{\{}LAG{\}} used henceforth. Theoretically, the merits of this contribution are: i) the convergence rate is the same as batch gradient descent in strongly-convex, convex, and nonconvex smooth cases; and, ii) if the distributed datasets are heterogeneous (quantified by certain measurable constants), the communication rounds needed to achieve a targeted accuracy are reduced thanks to the adaptive reuse of $\backslash$emph{\{}lagged{\}} gradients. Numerical experiments on both synthetic and real data corroborate a significant communication reduction compared to alternatives.},
archivePrefix = {arXiv},
arxivId = {1805.09965},
author = {Chen, Tianyi and Giannakis, Georgios B. and Sun, Tao and Yin, Wotao},
eprint = {1805.09965},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Chen et al.{\_}2018{\_}LAG Lazily Aggregated Gradient for Communication-Efficient Distributed Learning.pdf:pdf},
title = {{LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning}},
url = {https://arxiv.org/abs/1805.09965{\%}0Ahttp://arxiv.org/abs/1805.09965},
year = {2018}
}
@article{Wang2016,
abstract = {We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a sepa-rate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the pre-dictor matrix has low rank. We consider a setting where each task is handled by a dif-ferent machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploit-ing the shared structure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.02185v1},
author = {Wang, Jialei and Kolar, Mladen and Srebro, Nathan},
eprint = {arXiv:1603.02185v1},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Wang, Kolar, Srebro{\_}2016{\_}Distributed Multi-Task Learning with Shared Representation.pdf:pdf},
keywords = {()},
title = {{Distributed Multi-Task Learning with Shared Representation}},
url = {https://arxiv.org/pdf/1603.02185.pdf},
year = {2016}
}
@article{Griffin2007,
abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 1 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) aftifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, the benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching 2 algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
author = {Griffin, G and Holub, a and Perona, P},
file = {:Users/eisamar/Documents/Mendeley/Caltech mimeo/Griffin, Holub, Perona{\_}2007{\_}Caltech-256 object category dataset.pdf:pdf},
isbn = {UCB/CSD-04-1366},
journal = {Caltech mimeo},
number = {1},
pages = {20},
title = {{Caltech-256 object category dataset}},
url = {http://authors.library.caltech.edu/7694},
volume = {11},
year = {2007}
}
@article{Konecny2017,
abstract = {We study optimization algorithms for the finite sum problems frequently arising in machine learning applications. First, we propose novel variants of stochastic gradient descent with a variance reduction property that enables linear convergence for strongly convex objectives. Second, we study distributed setting, in which the data describing the optimization problem does not fit into a single computing node. In this case, traditional methods are inefficient, as the communication costs inherent in distributed optimization become the bottleneck. We propose a communication-efficient framework which iteratively forms local subproblems that can be solved with arbitrary local optimization algorithms. Finally, we introduce the concept of Federated Optimization/Learning, where we try to solve the machine learning problems without having data stored in any centralized manner. The main motivation comes from industry when handling user-generated data. The current prevalent practice is that companies collect vast amounts of user data and store them in datacenters. An alternative we propose is not to collect the data in first place, and instead occasionally use the computational power of users' devices to solve the very same optimization problems, while alleviating privacy concerns at the same time. In such setting, minimization of communication rounds is the primary goal, and we demonstrate that solving the optimization problems in such circumstances is conceptually tractable.},
archivePrefix = {arXiv},
arxivId = {1707.01155},
author = {Kone{\v{c}}n{\'{y}}, Jakub},
eprint = {1707.01155},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Kone{\v{c}}n{\'{y}}{\_}2017{\_}Stochastic, Distributed and Federated Optimization for Machine Learning.pdf:pdf},
title = {{Stochastic, Distributed and Federated Optimization for Machine Learning}},
url = {http://arxiv.org/abs/1707.01155},
year = {2017}
}
@inproceedings{Evgeniou2004,
abstract = {Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.},
address = {New York, New York, USA},
author = {Evgeniou, Theodoros and Pontil, Massimiliano},
booktitle = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04},
doi = {10.1145/1014052.1014067},
file = {:Users/eisamar/Documents/Mendeley/Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04/Evgeniou, Pontil{\_}2004{\_}Regularized multi--task learning.pdf:pdf},
isbn = {1581138889},
pages = {109},
publisher = {ACM Press},
title = {{Regularized multi--task learning}},
url = {http://portal.acm.org/citation.cfm?doid=1014052.1014067},
year = {2004}
}
@incollection{NIPS2017_7029,
author = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {4427--4437},
publisher = {Curran Associates, Inc.},
title = {{Federated Multi-Task Learning}},
url = {http://papers.nips.cc/paper/7029-federated-multi-task-learning.pdf},
year = {2017}
}
@inproceedings{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Neural Information Processing Systems (NIPS)},
eprint = {1411.1792},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Yosinski et al.{\_}2014{\_}How transferable are features in deep neural networks.pdf:pdf},
issn = {10495258},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {27},
year = {2014}
}
@article{Wei2017,
abstract = {Transfer learning borrows knowledge from a source domain to facilitate learning in a target domain. Two primary issues to be addressed in transfer learning are what and how to transfer. For a pair of domains, adopting different transfer learning algorithms results in different knowledge transferred between them. To discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain, researchers have to exhaustively explore all existing transfer learning algorithms, which is computationally intractable. As a trade-off, a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we first learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function. Extensive experiments demonstrate the L2T's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge.},
archivePrefix = {arXiv},
arxivId = {1708.05629},
author = {Wei, Ying and Zhang, Yu and Yang, Qiang},
eprint = {1708.05629},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Wei, Zhang, Yang{\_}2017{\_}Learning to Transfer.pdf:pdf},
keywords = {Transfer Learning},
title = {{Learning to Transfer}},
url = {http://arxiv.org/abs/1708.05629},
year = {2017}
}
@article{Wang,
abstract = {We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning. We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.},
author = {Wang, Weiran and Wang, Jialei and Kolar, Mladen and Srebro, Nathan},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Wang et al.{\_}Unknown{\_}Distributed Stochastic Multi-Task Learning with Graph Regularization.pdf:pdf},
title = {{Distributed Stochastic Multi-Task Learning with Graph Regularization}},
url = {https://arxiv.org/pdf/1802.03830.pdf}
}
@article{Povey2014,
abstract = {We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.},
archivePrefix = {arXiv},
arxivId = {1410.7455},
author = {Povey, Daniel and Zhang, Xiaohui and Khudanpur, Sanjeev},
eprint = {1410.7455},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Povey, Zhang, Khudanpur{\_}2014{\_}Parallel training of DNNs with Natural Gradient and Parameter Averaging.pdf:pdf},
pages = {1--28},
title = {{Parallel training of DNNs with Natural Gradient and Parameter Averaging}},
url = {http://arxiv.org/abs/1410.7455},
year = {2014}
}
@article{Zhu2018,
abstract = {This paper studies the problem of nonparametric estimation of a smooth function with data distributed across multiple machines. We assume an independent sample from a white noise model is collected at each machine, and an estimator of the underlying true function needs to be constructed at a central machine. We place limits on the number of bits that each machine can use to transmit information to the central machine. Our results give both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. We identify three regimes, depending on the relationship among the number of machines, the size of the data available at each machine, and the communication budget. When the communication budget is small, the statistical risk depends solely on this communication bottleneck, regardless of the sample size. In the regime where the communication budget is large, the classic minimax risk in the non-distributed estimation setting is recovered. In an intermediate regime, the statistical risk depends on both the sample size and the communication budget.},
archivePrefix = {arXiv},
arxivId = {1803.01302},
author = {Zhu, Yuancheng and Lafferty, John},
eprint = {1803.01302},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Zhu, Lafferty{\_}2018{\_}Distributed Nonparametric Regression under Communication Constraints.pdf:pdf},
month = {mar},
title = {{Distributed Nonparametric Regression under Communication Constraints}},
url = {http://arxiv.org/abs/1803.01302},
year = {2018}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
file = {:Users/eisamar/Documents/Mendeley/{\ldots} Science Department, University of Toronto, Tech. {\ldots}/Krizhevsky{\_}2009{\_}Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
pages = {1--60},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {https://www.cs.toronto.edu/{~}kriz/learning-features-2009-TR.pdf},
year = {2009}
}
@article{Wichrowska2017,
abstract = {Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on. We release an open source implementation of the meta-training algorithm.},
archivePrefix = {arXiv},
arxivId = {1703.04813},
author = {Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha},
eprint = {1703.04813},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Wichrowska et al.{\_}2017{\_}Learned Optimizers that Scale and Generalize.pdf:pdf},
month = {mar},
title = {{Learned Optimizers that Scale and Generalize}},
url = {http://arxiv.org/abs/1703.04813},
year = {2017}
}
@inproceedings{Konecny2016a,
abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1610.05492},
author = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt{\'{a}}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
booktitle = {Proc. Neural Information Processing System Conference (NIPS'17) Workshop on Private Multi-Party Machine Learning},
eprint = {1610.05492},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Kone{\v{c}}n et al.{\_}Unknown{\_}FEDERATED LEARNING STRATEGIES FOR IMPROVING COMMUNICATION EFFICIENCY.pdf:pdf},
month = {oct},
pages = {1--10},
title = {{Federated Learning: Strategies for Improving Communication Efficiency}},
url = {http://arxiv.org/abs/1610.05492},
year = {2016}
}
@article{Meyerson2018,
abstract = {Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation is shown to improve performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.},
archivePrefix = {arXiv},
arxivId = {1803.04062},
author = {Meyerson, Elliot and Miikkulainen, Risto},
eprint = {1803.04062},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Meyerson, Miikkulainen{\_}2018{\_}Pseudo-task Augmentation From Deep Multitask Learning to Intratask Sharing---and Back.pdf:pdf},
month = {mar},
title = {{Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back}},
url = {http://arxiv.org/abs/1803.04062},
year = {2018}
}
@article{Ruder2017,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Ruder{\_}2017{\_}An Overview of Multi-Task Learning in Deep Neural Networks.pdf:pdf},
number = {May},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{Mishra2017,
abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
archivePrefix = {arXiv},
arxivId = {1707.03141},
author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
eprint = {1707.03141},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Mishra et al.{\_}2017{\_}A Simple Neural Attentive Meta-Learner.pdf:pdf},
month = {jul},
title = {{A Simple Neural Attentive Meta-Learner}},
url = {http://arxiv.org/abs/1707.03141},
year = {2017}
}
@misc{Hotelling1936,
author = {Hotelling, Harold (Columbia University)},
booktitle = {Biometrika},
file = {:Users/eisamar/Documents/Mendeley/Biometrika/Hotelling{\_}1936{\_}Relations between two sets of variates.pdf:pdf},
number = {3/4},
pages = {321--377},
publisher = {Biometrika Trust},
title = {{Relations between two sets of variates}},
url = {http://cbio.ensmp.fr/{~}jvert/svn/bibli/local/Hotelling1936Relation.pdf},
volume = {28},
year = {1936}
}
@article{Zhao2018a,
abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55{\%} for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30{\%} for the CIFAR-10 dataset with only 5{\%} globally shared data.},
archivePrefix = {arXiv},
arxivId = {1806.00582},
author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
eprint = {1806.00582},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Zhao et al.{\_}2018{\_}Federated Learning with Non-IID Data.pdf:pdf},
title = {{Federated Learning with Non-IID Data}},
url = {http://arxiv.org/abs/1806.00582},
year = {2018}
}
@article{Liu2016,
abstract = {Multi-task learning aims to learn multiple tasks jointly by exploiting their relatedness to improve the generalization performance for each task. Traditionally, to perform multi-task learning, one needs to centralize data from all the tasks to a single machine. However, in many real-world applications, data of different tasks may be geo-distributed over different local machines. Due to heavy communication caused by transmitting the data and the issue of data privacy and security, it is impossible to send data of different task to a master machine to perform multi-task learning. Therefore, in this paper, we propose a distributed multi-task learning framework that simultaneously learns predictive models for each task as well as task relationships between tasks alternatingly in the parameter server paradigm. In our framework, we first offer a general dual form for a family of regularized multi-task relationship learning methods. Subsequently, we propose a communication-efficient primal-dual distributed optimization algorithm to solve the dual problem by carefully designing local subproblems to make the dual problem decomposable. Moreover, we provide a theoretical convergence analysis for the proposed algorithm, which is specific for distributed multi-task relationship learning. We conduct extensive experiments on both synthetic and real-world datasets to evaluate our proposed framework in terms of effectiveness and convergence.},
archivePrefix = {arXiv},
arxivId = {1612.04022},
author = {Liu, Sulin and Pan, Sinno Jialin and Ho, Qirong},
doi = {10.1145/3097983.3098136},
eprint = {1612.04022},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Liu, Pan, Ho{\_}2016{\_}Distributed Multi-Task Relationship Learning.pdf:pdf},
isbn = {9781450348874},
month = {dec},
title = {{Distributed Multi-Task Relationship Learning}},
url = {http://arxiv.org/abs/1612.04022},
year = {2016}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and de Freitas, Nando},
doi = {10.1007/s10115-008-0151-5},
eprint = {1606.04474},
file = {:Users/eisamar/Documents/Mendeley/arXiv1606.04474 cs/Andrychowicz et al.{\_}2016{\_}Learning to learn by gradient descent by gradient descent.pdf:pdf},
isbn = {1011500801515},
issn = {0219-1377},
journal = {arXiv:1606.04474 [cs]},
number = {Nips},
pages = {1--17},
pmid = {207591},
title = {{Learning to learn by gradient descent by gradient descent}},
url = {https://arxiv.org/pdf/1606.04474.pdf http://arxiv.org/abs/1606.04474},
year = {2016}
}
@article{eitz2012hdhso,
author = {Eitz, Mathias and Hays, James and Alexa, Marc},
journal = {ACM Trans. Graph. (Proc. SIGGRAPH)},
number = {4},
pages = {44:1----44:10},
title = {{How Do Humans Sketch Objects?}},
volume = {31},
year = {2012}
}
@article{Konecny2015,
abstract = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed (unevenly) over an extremely large number of $\backslash$nodes, but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of utmost importance. A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead, the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network, each of which has only a tiny fraction of data available totally; in particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, we assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization.},
archivePrefix = {arXiv},
arxivId = {1511.03575},
author = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, Brendan and Ramage, Daniel},
eprint = {1511.03575},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Kone{\v{c}}n{\'{y}}, McMahan, Ramage{\_}2015{\_}Federated OptimizationDistributed Optimization Beyond the Datacenter.pdf:pdf},
number = {1},
pages = {1--5},
title = {{Federated Optimization:Distributed Optimization Beyond the Datacenter}},
url = {http://arxiv.org/abs/1511.03575},
year = {2015}
}
@article{Argyriou2008,
abstract = {We present a method for learning sparse representations shared acrossmultiple tasks. This method is a generalization of the well-known single- task 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a super- vised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse repre- sentations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report exper- iments on simulated and real data sets which demonstrate that the proposed method can both improve the performance relative to learning each task in- dependently and lead to a few learned features common across related tasks. Our algorithm can also be used, as a special case, to simply select – not learn – a few common variables across the tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.5753v5},
author = {Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
doi = {10.1007/s10994-007-5040-8},
eprint = {arXiv:1203.5753v5},
file = {:Users/eisamar/Documents/Mendeley/Machine Learning/Argyriou, Evgeniou, Pontil{\_}2008{\_}Convex multi-task feature learning.pdf:pdf},
isbn = {0885-61250885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Collaborative filtering,Inductive transfer,Kernels,Multi-task learning,Regularization,Transfer learning,Vector-valued functions},
month = {dec},
number = {3},
pages = {243--272},
pmid = {3657163},
publisher = {Springer US},
title = {{Convex multi-task feature learning}},
url = {http://link.springer.com/10.1007/s10994-007-5040-8},
volume = {73},
year = {2008}
}
@article{liaw2018tune,
author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
journal = {arXiv preprint arXiv:1807.05118},
title = {{Tune: A Research Platform for Distributed Model Selection and Training}},
year = {2018}
}
@inproceedings{zhang2015beyond,
author = {Zhang, Ning and Paluri, Manohar and Taigman, Yaniv and Fergus, Rob and Bourdev, Lubomir},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {4804--4813},
title = {{Beyond frontal faces: Improving person recognition using multiple cues}},
url = {https://people.eecs.berkeley.edu/{~}nzhang/piper.html},
year = {2015}
}
@inproceedings{rahman2015unintrusive,
author = {Rahman, Shah Atiqur and Merck, Christopher and Huang, Yuxiao and Kleinberg, Samantha},
booktitle = {Proceedings of the 9th International Conference on Pervasive Computing Technologies for Healthcare},
organization = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
pages = {108--111},
title = {{Unintrusive eating recognition using Google Glass}},
year = {2015}
}
@article{Konecny2016,
abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1610.05492},
author = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt{\'{a}}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
eprint = {1610.05492},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Kone{\v{c}}n et al.{\_}Unknown{\_}FEDERATED LEARNING STRATEGIES FOR IMPROVING COMMUNICATION EFFICIENCY.pdf:pdf},
month = {oct},
title = {{Federated Learning: Strategies for Improving Communication Efficiency}},
url = {http://arxiv.org/abs/1610.05492},
year = {2016}
}
@article{Cheng2016,
abstract = {Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide {\&} Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide {\&} Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
archivePrefix = {arXiv},
arxivId = {1606.07792},
author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
doi = {10.1145/2988450.2988454},
eprint = {1606.07792},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Cheng et al.{\_}2016{\_}Wide {\&} Deep Learning for Recommender Systems.pdf:pdf},
isbn = {9781450347952},
issn = {10450823},
keywords = {deep learning,recommender systems,wide},
pages = {1--4},
pmid = {142791},
title = {{Wide {\&} Deep Learning for Recommender Systems}},
url = {http://arxiv.org/abs/1606.07792},
year = {2016}
}
@article{Orekondy2018,
abstract = {Machine Learning techniques are widely used by online services (e.g. Google, Apple) in order to analyze and make predictions on user data. As many of the provided services are user-centric (e.g. personal photo collections, speech recognition, personal assistance), user data generated on personal devices is key to provide the service. In order to protect the data and the privacy of the user, federated learning techniques have been proposed where the data never leaves the user's device and "only" model updates are communicated back to the server. In our work, we propose a new threat model that is not concerned with learning about the content - but rather is concerned with the linkability of users during such decentralized learning scenarios. We show that model updates are characteristic for users and therefore lend themselves to linkability attacks. We show identification and matching of users across devices in closed and open world scenarios. In our experiments, we find our attacks to be highly effective, achieving 20x-175x chance-level performance. In order to mitigate the risks of linkability attacks, we study various strategies. As adding random noise does not offer convincing operation points, we propose strategies based on using calibrated domain-specific data; we find these strategies offers substantial protection against linkability threats with little effect to utility.},
archivePrefix = {arXiv},
arxivId = {1805.05838},
author = {Orekondy, Tribhuvanesh and Oh, Seong Joon and Schiele, Bernt and Fritz, Mario},
eprint = {1805.05838},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Orekondy et al.{\_}2018{\_}Understanding and Controlling User Linkability in Decentralized Learning.pdf:pdf},
keywords = {machine learning,privacy},
title = {{Understanding and Controlling User Linkability in Decentralized Learning}},
url = {http://arxiv.org/abs/1805.05838},
year = {2018}
}
@article{Tsitsulin,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.04742v1},
author = {Tsitsulin, Anton and Mottin, Davide and M{\"{u}}ller, Emmanuel},
eprint = {arXiv:1803.04742v1},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Tsitsulin, Mottin, M{\"{u}}ller{\_}Unknown{\_}VERSE Versatile Graph Embeddings from Similarity Measures.pdf:pdf},
isbn = {9781450356398},
title = {{VERSE : Versatile Graph Embeddings from Similarity Measures}}
}
@article{Zhang2014,
abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
archivePrefix = {arXiv},
arxivId = {1412.6651},
author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
eprint = {1412.6651},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Zhang, Choromanska, LeCun{\_}2014{\_}Deep learning with Elastic Averaging SGD.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Deep learning with Elastic Averaging SGD}},
url = {http://arxiv.org/abs/1412.6651},
year = {2014}
}
@article{Shen2018,
abstract = {Recently, the decentralized optimization problem is attracting growing attention. Most existing methods are deterministic with high per-iteration cost and have a convergence rate quadratically depending on the problem condition number. Besides, the dense communication is necessary to ensure the convergence even if the dataset is sparse. In this paper, we generalize the decentralized optimization problem to a monotone operator root finding problem, and propose a stochastic algorithm named DSBA that (i) converges geometrically with a rate linearly depending on the problem condition number, and (ii) can be implemented using sparse communication only. Additionally, DSBA handles learning problems like AUC-maximization which cannot be tackled efficiently in the decentralized setting. Experiments on convex minimization and AUC-maximization validate the efficiency of our method.},
archivePrefix = {arXiv},
arxivId = {1805.09969},
author = {Shen, Zebang and Mokhtari, Aryan and Zhou, Tengfei and Zhao, Peilin and Qian, Hui},
eprint = {1805.09969},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Shen et al.{\_}2018{\_}Towards More Efficient Stochastic Decentralized Learning Faster Convergence and Sparse Communication.pdf:pdf},
month = {may},
title = {{Towards More Efficient Stochastic Decentralized Learning: Faster Convergence and Sparse Communication}},
url = {http://arxiv.org/abs/1805.09969},
year = {2018}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
file = {::},
isbn = {0036-8075},
issn = {10959203},
journal = {Science},
month = {dec},
number = {6266},
pages = {1332--1338},
pmid = {26659050},
primaryClass = {10.1126},
publisher = {American Association for the Advancement of Science},
title = {{Human-level concept learning through probabilistic program induction}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26659050},
volume = {350},
year = {2015}
}
@article{Smith,
author = {Smith, Virginia and Cmu, Stanford},
file = {:Users/eisamar/Documents/Mendeley/Neural Information Processing Systems (NIPS)/Smith, Cmu{\_}2017{\_}MOCHA Federated Multi-Task Learning(2).pdf:pdf;:Users/eisamar/Documents/Mendeley/Neural Information Processing Systems (NIPS)/Smith, Cmu{\_}2017{\_}MOCHA Federated Multi-Task Learning.pdf:pdf},
journal = {Neural Information Processing Systems (NIPS)},
title = {{MOCHA : Federated Multi-Task Learning}},
year = {2017}
}
@article{Anil2018,
author = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E and Hinton, Geoffrey E},
file = {:Users/eisamar/Documents/Mendeley/Iclr/Anil et al.{\_}2018{\_}Large scale distributed neural network training through online distillation.pdf:pdf},
journal = {Iclr},
pages = {1--12},
title = {{Large scale distributed neural network training through online distillation}},
year = {2018}
}
@article{Wang2015,
abstract = {We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space,where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.},
archivePrefix = {arXiv},
arxivId = {1510.00633},
author = {Wang, Jialei and Kolar, Mladen and Srebro, Nathan},
eprint = {1510.00633},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Wang, Kolar, Srebro{\_}2015{\_}Distributed Multitask Learning.pdf:pdf},
title = {{Distributed Multitask Learning}},
url = {http://arxiv.org/abs/1510.00633},
year = {2015}
}
@article{Li2018,
abstract = {Deep learning is a promising approach for extracting accurate information from raw sen- sor data from IoT devices deployed in complex environments. Because of its multilayer structure, deep learning is also appropriate for the edge computing environment. Therefore, in this article, we first introduce deep learning for IoTs into the edge computing environment. Since existing edge nodes have limited processing capability, we also design a novel offloading strategy to optimize the performance of IoT deep learning applications with edge computing. In the performance evalu- ation, we test the performance of executing mul- tiple deep learning tasks in an edge computing environment with our strategy. The evaluation results show that our method outperforms other optimization solutions on deep learning for IoT.},
author = {Li, He and Ota, Kaoru and Dong, Mianxiong},
doi = {10.1109/MNET.2018.1700202},
file = {:Users/eisamar/Documents/Mendeley/IEEE Network/Li, Ota, Dong{\_}2018{\_}Learning IoT in Edge Deep Learning for the Internet of Things with Edge Computing.pdf:pdf},
issn = {08908044},
journal = {IEEE Network},
number = {1},
pages = {96--101},
title = {{Learning IoT in Edge: Deep Learning for the Internet of Things with Edge Computing}},
volume = {32},
year = {2018}
}
@inproceedings{blitzer2007biographies,
author = {Blitzer, John and Dredze, Mark and Pereira, Fernando},
booktitle = {Proceedings of the 45th annual meeting of the association of computational linguistics},
pages = {440--447},
title = {{Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
year = {2007}
}
@article{Duarte2004,
abstract = {The task of classifying the types of moving vehicles in a distributed, wireless sensor network is investigated. Specifically, based on an extensive real world experiment, we have compiled a data set that consists of 820 MByte raw time series data, 70 MByte of preprocessed, extracted spectral feature vectors, and baseline classification results using the maximum likelihood classifier. The purpose of this paper is to detail the data collection procedure, the feature extraction and pre-processing steps, and baseline classifier development. The database is available for download at http://www.ece.wisc.edu/∼sensit starting on July 2003. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Duarte, Marco F. and Hu, Yu Hen},
doi = {10.1016/j.jpdc.2004.03.020},
file = {:Users/eisamar/Documents/Mendeley/Journal of Parallel and Distributed Computing/Duarte, Hu{\_}2004{\_}Vehicle classification in distributed sensor networks.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
number = {7},
pages = {826--838},
title = {{Vehicle classification in distributed sensor networks}},
volume = {64},
year = {2004}
}
@article{Prieto2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.07016v1},
author = {Cano, Ignacio and Weimer, Markus and Mahajan, Dhruv and Curino, Carlo and {Matteo Fumarola}, Giovanni},
doi = {10.1145/1235},
eprint = {arXiv:1603.07016v1},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Cano et al.{\_}2017{\_}Towards Geo-Distributed Machine Learning.pdf:pdf},
isbn = {9781450321389},
issn = {16130073},
keywords = {Multimodal learning analytics,Multimodal teaching analytics,STEM education,Sensors,Smart classroom,Smart school},
pages = {53--59},
title = {{Towards Geo-Distributed Machine Learning}},
volume = {1828},
year = {2017}
}
@article{RichCaruana1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Rich Caruana} and Caruana, Rich and Mitchell, Tom and Pomerleau, Dean and Dietterich, Tom and State, Oregon},
doi = {10.1023/A:1007379606734},
eprint = {arXiv:1011.1669v3},
file = {:Users/eisamar/Documents/Mendeley/Machine Learning/Rich Caruana et al.{\_}1997{\_}Multitask Learning.pdf:pdf;:Users/eisamar/Documents/Mendeley/Machine Learning/Rich Caruana et al.{\_}1997{\_}Multitask Learning.pdf:pdf},
isbn = {1461375274},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,kernel regression,multitask learning,parallel transfer,regression,supervised learning},
number = {September},
pages = {41--75},
pmid = {20421687},
publisher = {Springer},
title = {{Multitask Learning *}},
volume = {28},
year = {1997}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
doi = {10.1017/S0140525X16001837},
eprint = {1706.03762},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Vaswani et al.{\_}2017{\_}Attention Is All You Need.pdf:pdf},
isbn = {9781577357384},
issn = {0140-525X},
pages = {5998--6008},
pmid = {1000303116},
title = {{Attention Is All You Need}},
url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need http://arxiv.org/abs/1706.03762},
year = {2017}
}
@article{Pan2009,
abstract = {—A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
file = {:Users/eisamar/Documents/Mendeley/Ieee Transactions on Knowledge and Data Engineering/Pan, Yang{\_}2009{\_}A Survey on Transfer Learning.pdf:pdf},
journal = {Ieee Transactions on Knowledge and Data Engineering},
pages = {1--15},
title = {{A Survey on Transfer Learning}},
url = {https://www.cse.ust.hk/{~}qyang/Docs/2009/tkde{\_}transfer{\_}learning.pdf},
year = {2009}
}
@article{Shamir2014,
abstract = {We consider the problem of distributed stochastic optimization, where each of several machines has access to samples from the same source distribution, and the goal is to jointly optimize the expected objective w.r.t. the source distribution, minimizing: (1) overall runtime; (2) communication costs; (3) number of samples used. We study this problem systematically, highlighting fundamental limitations, and differences versus distributed consensus problems where each machine has a different, independent, objective. We show how the best known guarantees are obtained by an accelerated mini-batched SGD approach, and contrast the runtime and sample costs of the approach with those of other distributed optimization algorithms.},
author = {Shamir, Ohad and Srebro, Nathan},
doi = {10.1109/ALLERTON.2014.7028543},
file = {:Users/eisamar/Documents/Mendeley/2014 52nd Annual Allerton Conference on Communication, Control, and Computing, Allerton 2014/Shamir, Srebro{\_}2014{\_}Distributed stochastic optimization and learning.pdf:pdf},
isbn = {9781479980093},
journal = {2014 52nd Annual Allerton Conference on Communication, Control, and Computing, Allerton 2014},
pages = {850--857},
title = {{Distributed stochastic optimization and learning}},
url = {http://ttic.uchicago.edu/{~}nati/Publications/ShamirSrebro2014Allerton.pdf},
year = {2014}
}
@article{Tang2018,
abstract = {While training a machine learning model using multiple workers, each of which collects data from their own data sources, it would be most useful when the data collected from different workers can be {\{}$\backslash$em unique{\}} and {\{}$\backslash$em different{\}}. Ironically, recent analysis of decentralized parallel stochastic gradient descent (D-PSGD) relies on the assumption that the data hosted on different workers are {\{}$\backslash$em not too different{\}}. In this paper, we ask the question: {\{}$\backslash$em Can we design a decentralized parallel stochastic gradient descent algorithm that is less sensitive to the data variance across workers?{\}} In this paper, we present D{\$}{\^{}}2{\$}, a novel decentralized parallel stochastic gradient descent algorithm designed for large data variance $\backslash$xr{\{}among workers{\}} (imprecisely, "decentralized" data). The core of D{\$}{\^{}}2{\$} is a variance blackuction extension of the standard D-PSGD algorithm, which improves the convergence rate from {\$}O\backslashleft({\{}\backslashsigma \backslashover \backslashsqrt{\{}nT{\}}{\}} + {\{}(n\backslashzeta{\^{}}2){\^{}}{\{}\backslashfrac{\{}1{\}}{\{}3{\}}{\}} \backslashover T{\^{}}{\{}2/3{\}}{\}}\backslashright){\$} to {\$}O\backslashleft({\{}\backslashsigma \backslashover \backslashsqrt{\{}nT{\}}{\}}\backslashright){\$} where {\$}\backslashzeta{\^{}}{\{}2{\}}{\$} denotes the variance among data on different workers. As a result, D{\$}{\^{}}2{\$} is robust to data variance among workers. We empirically evaluated D{\$}{\^{}}2{\$} on image classification tasks where each worker has access to only the data of a limited set of labels, and find that D{\$}{\^{}}2{\$} significantly outperforms D-PSGD.},
archivePrefix = {arXiv},
arxivId = {1803.07068},
author = {Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
eprint = {1803.07068},
file = {:Users/eisamar/Documents/Mendeley/Unknown/Tang et al.{\_}2018{\_}D{\$}2{\$} Decentralized Training over Decentralized Data.pdf:pdf},
month = {mar},
title = {{D{\$}{\^{}}2{\$}: Decentralized Training over Decentralized Data}},
url = {http://arxiv.org/abs/1803.07068},
year = {2018}
}
